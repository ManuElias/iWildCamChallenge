{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "MegaDetector_V4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zh2oyVvVvpT"
      },
      "source": [
        "# Megadetector V4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss0gfjBrVvpZ"
      },
      "source": [
        "Dieses Notebook enthält einen Object Detector für Objekte der Klasse Mensch (human) und Tier (animal)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ex_lmSI3Vvpa"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZodTrdCVvpc"
      },
      "source": [
        "Für die Einbindung des Object Detectors wird die ältere TensorFlow-Version 1.13.1 benötigt.\n",
        "Dafür wird die aktuelle Version deinstalliert.\n",
        "\n",
        "Die Funktionen und Klassen aus dem Package *humanfriendly* können verwendet werden, um Textschnittstellen benutzerfreundlicher zu gestalten.\n",
        "\n",
        "Die Pythonlibrary *jsonpickle* dient zur Serialisierung und Deserialisierung von komplexen Python-Objekten in und aus JSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_pMX51AJVvpd",
        "outputId": "1172956a-038c-4cda-a86c-09bda17ec04f"
      },
      "source": [
        "!pip uninstall tensorflow-y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow-y as it is not installed.\u001b[0m\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FLSQ_2GYVvpe",
        "outputId": "1e10d542-4d16-4a41-99d0-53115d1ce3d9"
      },
      "source": [
        "!pip install tensorflow==1.13.1 humanfriendly jsonpickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /opt/conda/lib/python3.7/site-packages (1.13.1)\n",
            "Requirement already satisfied: humanfriendly in /opt/conda/lib/python3.7/site-packages (9.1)\n",
            "Requirement already satisfied: jsonpickle in /opt/conda/lib/python3.7/site-packages (1.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (3.14.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "V4y4ml0kVvpf",
        "outputId": "e1bc4a56-9f03-4882-caba-c3e0619533bf"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bPaFx7uLVvpg",
        "outputId": "a20c0108-27d1-464b-f8cf-4019bd8d9013"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loa1BME7Vvph"
      },
      "source": [
        "Die neuste Version des MegaDetector Modells wird über den Link [md_v4.1.0.pb](https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb) gedownloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XFXoj7ElVvpi",
        "outputId": "64fd91ca-cdf8-4372-e676-c0fc26e3c325"
      },
      "source": [
        "\n",
        "!wget -O megadetector_v4_1_0.pb \"https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-23 14:25:29--  https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb\n",
            "Resolving lilablobssc.blob.core.windows.net (lilablobssc.blob.core.windows.net)... 52.239.159.84\n",
            "Connecting to lilablobssc.blob.core.windows.net (lilablobssc.blob.core.windows.net)|52.239.159.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245590501 (234M) [application/octet-stream]\n",
            "Saving to: ‘megadetector_v4_1_0.pb’\n",
            "\n",
            "megadetector_v4_1_0 100%[===================>] 234.21M   932KB/s    in 3m 45s  \n",
            "\n",
            "2021-02-23 14:29:15 (1.04 MB/s) - ‘megadetector_v4_1_0.pb’ saved [245590501/245590501]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x68JEbgVvpj"
      },
      "source": [
        "Für ein einfacheres Datenhandling und die Ausführung des Modells werden die geforderten GitHub Repositories geclont.\n",
        "\n",
        "Benötigt werden die neuste Version der Microsoft AI for Earth \"utilities\" und \"Camera Traps\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "47F-wdMCVvpj",
        "outputId": "ac5ecd8f-292e-4de4-87fc-3335fcc70ac3"
      },
      "source": [
        "!git clone https://github.com/microsoft/CameraTraps/\n",
        "!git clone https://github.com/microsoft/ai4eutils/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'CameraTraps' already exists and is not an empty directory.\n",
            "fatal: destination path 'ai4eutils' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HbuFLKfSVvpk",
        "outputId": "14920633-d8af-438f-9bfd-faf580f160b5"
      },
      "source": [
        "print(os.listdir(os.getcwd()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['run_tf_detector_batch.py', 'megadetector_v4_1_0.pb', 'ai4eutils', 'output.json', '__notebook_source__.ipynb', 'visualize_detector_output.py', 'CameraTraps', 'images']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkNejycAVvpk"
      },
      "source": [
        "Außerdem werden die Pythonskripte für die Ausführung des Modells und die Visualisierung der Ergebnisse in das Working Directory kopiert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ALoOrr38Vvpl"
      },
      "source": [
        "!cp /kaggle/working/CameraTraps/detection/run_tf_detector_batch.py .\n",
        "!cp /kaggle/working/CameraTraps/visualization/visualize_detector_output.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZofQNEcVvpl"
      },
      "source": [
        "Um später die Module aus jedem Arbeitsverzeichnis importieren zu können, werden die geklonten GitHub-Ordner zur Umgebungsvariable PYTHONPATH hinzugefügt. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4m_-x8J3Vvpm",
        "outputId": "be43ca0e-4cd0-442c-f4bf-7d18859b87b0"
      },
      "source": [
        "os.environ['PYTHONPATH'] += \":/kaggle/working/ai4eutils\"\n",
        "os.environ['PYTHONPATH'] += \":/kaggle/working/CameraTraps\"\n",
        "\n",
        "!echo \"PYTHONPATH: $PYTHONPATH\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PYTHONPATH: /kaggle/lib/kagglegym:/kaggle/lib:/kaggle/input/iwildcam-2020-fgvc7:/kaggle/working/ai4eutils:/kaggle/working/CameraTraps\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxnswkvHVvpn"
      },
      "source": [
        "# Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FyRbuYeVvpn"
      },
      "source": [
        "Da der Detector etwa 1 Sekunde pro Bild benötigt, das heißt, etwa 3600 Bilder pro Stunde verarbeiten kann, wird er  im Folgenden beispielhaft auf 20 Testbilder angewandt.\n",
        "\n",
        "Um das Pythonskript [run_tf_detector_batch.py](https://github.com/microsoft/CameraTraps/blob/master/detection/run_tf_detector_batch.py) aus dem Camera Traps Repository auszuführen, sind folgende Argumente nötig:\n",
        "1. Einen Ordner mit den Bildern, auf denen die Object Detection ausgeführt werden soll\n",
        "2. Eine Outputdatei in JSON Format, welche die ermittelten Bilddaten enthält.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Cd9SOSNVvpo"
      },
      "source": [
        "images_dir = '../input/iwildcam-2020-fgvc7/test'\n",
        "\n",
        "test_images = os.listdir(images_dir)\n",
        "#test batch\n",
        "test_images_batch = test_images[:20] \n",
        "\n",
        "for image in test_images_batch:\n",
        "    !cp ../input/iwildcam-2020-fgvc7/test/{image} /kaggle/working/images\n",
        "    \n",
        "# choose a location for the output JSON file\n",
        "output_file_path = '/output/kaggle/working/output.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "atC_8cNUVvpp"
      },
      "source": [
        "os.chdir(\"/kaggle/working\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4xOSNy_GVvpp"
      },
      "source": [
        "images_directory = '/kaggle/working/images/'\n",
        "# choose a location for the output JSON file\n",
        "output_file_path = '/kaggle/working/output.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWD4-uOKVvpp"
      },
      "source": [
        "Im nächsten Schritt wird die Object Detection ausgeführt. Dabei wird der Wert der Variable output_file_path übergeben, um sich später zur Visualisierung auf den Pfad des Outputfile beziehen zu können.\n",
        "\n",
        "Das Modell basiert auf einem Faster-RCNN mit einem Inception ResNet v2 Basisnetzwerk. Das ist eine konvolutionale neuronale Architektur, die auf der Inception-Familie von Architekturen aufbaut, aber residuale Verbindungen einbezieht und damit die Filterverkettungsstufe der Inception-Architektur ersetzt.\n",
        "Das Modell wurde mit der TensorFlow Object Detection API trainiert. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qJw9Y4nMVvpq",
        "outputId": "ea72607a-1ca3-408c-f884-0b9923f6dc35"
      },
      "source": [
        "#run megadetector\n",
        "!python run_tf_detector_batch.py megadetector_v4_1_0.pb \"$images_directory\" \"$output_file_path\" --recursive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.13.1\n",
            "2021-02-23 14:35:25.256370: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-02-23 14:35:25.275185: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-02-23 14:35:25.275771: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x557a1a1bd730 executing computations on platform Host. Devices:\n",
            "2021-02-23 14:35:25.275823: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "Is GPU available? tf.test.is_gpu_available: False\n",
            "TensorFlow version: 1.13.1\n",
            "tf.test.is_gpu_available: False\n",
            "Warning: output_file /kaggle/working/output.json already exists and will be overwritten\n",
            "40 image files found in the input directory\n",
            "TFDetector: Loading graph...\n",
            "TFDetector: Detection graph loaded.\n",
            "Loaded model in 9.46 seconds\n",
            "  0%|                                                    | 0/40 [00:00<?, ?it/s]Processing image /kaggle/working/images/anno_~kaggle~working~images~8c712ba8-21bc-11ea-a13a-137349068a90.jpg\n",
            "2021-02-23 14:35:45.074136: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0\n",
            "2021-02-23 14:35:50.889138: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0\n",
            "  2%|█                                           | 1/40 [00:46<30:12, 46.49s/it]Processing image /kaggle/working/images/905cdb0e-21bc-11ea-a13a-137349068a90.jpg\n",
            "  5%|██▏                                         | 2/40 [01:16<23:28, 37.06s/it]Processing image /kaggle/working/images/9310face-21bc-11ea-a13a-137349068a90.jpg\n",
            "  8%|███▎                                        | 3/40 [01:48<21:09, 34.32s/it]Processing image /kaggle/working/images/8e01cbc6-21bc-11ea-a13a-137349068a90.jpg\n",
            " 10%|████▍                                       | 4/40 [02:18<19:42, 32.84s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~94cf8632-21bc-11ea-a13a-137349068a90.jpg\n",
            " 12%|█████▌                                      | 5/40 [02:46<18:06, 31.05s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~89387e28-21bc-11ea-a13a-137349068a90.jpg\n",
            " 15%|██████▌                                     | 6/40 [03:14<16:57, 29.91s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~8e4efa86-21bc-11ea-a13a-137349068a90.jpg\n",
            " 18%|███████▋                                    | 7/40 [03:44<16:35, 30.15s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~9399b1ca-21bc-11ea-a13a-137349068a90.jpg\n",
            " 20%|████████▊                                   | 8/40 [04:12<15:40, 29.38s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~88eb0e86-21bc-11ea-a13a-137349068a90.jpg\n",
            " 22%|█████████▉                                  | 9/40 [04:40<14:54, 28.84s/it]Processing image /kaggle/working/images/873d13ae-21bc-11ea-a13a-137349068a90.jpg\n",
            " 25%|██████████▊                                | 10/40 [05:08<14:19, 28.66s/it]Processing image /kaggle/working/images/9526e594-21bc-11ea-a13a-137349068a90.jpg\n",
            " 28%|███████████▊                               | 11/40 [05:39<14:10, 29.32s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~938b7ad8-21bc-11ea-a13a-137349068a90.jpg\n",
            " 30%|████████████▉                              | 12/40 [06:06<13:25, 28.77s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~8a3d4e2a-21bc-11ea-a13a-137349068a90.jpg\n",
            " 32%|█████████████▉                             | 13/40 [06:37<13:14, 29.44s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~8e01cbc6-21bc-11ea-a13a-137349068a90.jpg\n",
            " 35%|███████████████                            | 14/40 [07:08<12:52, 29.73s/it]Processing image /kaggle/working/images/88eb0e86-21bc-11ea-a13a-137349068a90.jpg\n",
            " 38%|████████████████▏                          | 15/40 [07:36<12:10, 29.22s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~957f9414-21bc-11ea-a13a-137349068a90.jpg\n",
            " 40%|█████████████████▏                         | 16/40 [08:04<11:31, 28.81s/it]Processing image /kaggle/working/images/8d238c30-21bc-11ea-a13a-137349068a90.jpg\n",
            " 42%|██████████████████▎                        | 17/40 [08:32<11:02, 28.79s/it]Processing image /kaggle/working/images/94cf8632-21bc-11ea-a13a-137349068a90.jpg\n",
            " 45%|███████████████████▎                       | 18/40 [09:00<10:27, 28.52s/it]Processing image /kaggle/working/images/8e4efa86-21bc-11ea-a13a-137349068a90.jpg\n",
            " 48%|████████████████████▍                      | 19/40 [09:31<10:12, 29.14s/it]Processing image /kaggle/working/images/89387e28-21bc-11ea-a13a-137349068a90.jpg\n",
            " 50%|█████████████████████▌                     | 20/40 [09:58<09:33, 28.69s/it]Processing image /kaggle/working/images/8a3d4e2a-21bc-11ea-a13a-137349068a90.jpg\n",
            " 52%|██████████████████████▌                    | 21/40 [10:29<09:15, 29.25s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~926f87e8-21bc-11ea-a13a-137349068a90.jpg\n",
            " 55%|███████████████████████▋                   | 22/40 [10:56<08:36, 28.70s/it]Processing image /kaggle/working/images/890fff20-21bc-11ea-a13a-137349068a90.jpg\n",
            " 57%|████████████████████████▋                  | 23/40 [11:25<08:08, 28.73s/it]Processing image /kaggle/working/images/957f9414-21bc-11ea-a13a-137349068a90.jpg\n",
            " 60%|█████████████████████████▊                 | 24/40 [11:53<07:34, 28.41s/it]Processing image /kaggle/working/images/938b7ad8-21bc-11ea-a13a-137349068a90.jpg\n",
            " 62%|██████████████████████████▉                | 25/40 [12:20<07:02, 28.17s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~875e1de2-21bc-11ea-a13a-137349068a90.jpg\n",
            " 65%|███████████████████████████▉               | 26/40 [12:48<06:32, 28.06s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~8823b0b6-21bc-11ea-a13a-137349068a90.jpg\n",
            " 68%|█████████████████████████████              | 27/40 [13:17<06:05, 28.13s/it]Processing image /kaggle/working/images/89134a4a-21bc-11ea-a13a-137349068a90.jpg\n",
            " 70%|██████████████████████████████             | 28/40 [13:47<05:47, 28.93s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~905cdb0e-21bc-11ea-a13a-137349068a90.jpg\n",
            " 72%|███████████████████████████████▏           | 29/40 [14:18<05:24, 29.52s/it]Processing image /kaggle/working/images/926f87e8-21bc-11ea-a13a-137349068a90.jpg\n",
            " 75%|████████████████████████████████▎          | 30/40 [14:46<04:50, 29.05s/it]Processing image /kaggle/working/images/8c712ba8-21bc-11ea-a13a-137349068a90.jpg\n",
            " 78%|█████████████████████████████████▎         | 31/40 [15:14<04:19, 28.81s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~9526e594-21bc-11ea-a13a-137349068a90.jpg\n",
            " 80%|██████████████████████████████████▍        | 32/40 [15:45<03:54, 29.37s/it]Processing image /kaggle/working/images/9399b1ca-21bc-11ea-a13a-137349068a90.jpg\n",
            " 82%|███████████████████████████████████▍       | 33/40 [16:13<03:21, 28.84s/it]Processing image /kaggle/working/images/875e1de2-21bc-11ea-a13a-137349068a90.jpg\n",
            " 85%|████████████████████████████████████▌      | 34/40 [16:40<02:50, 28.48s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~8d238c30-21bc-11ea-a13a-137349068a90.jpg\n",
            " 88%|█████████████████████████████████████▋     | 35/40 [17:09<02:21, 28.38s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~873d13ae-21bc-11ea-a13a-137349068a90.jpg\n",
            " 90%|██████████████████████████████████████▋    | 36/40 [17:37<01:53, 28.31s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~9310face-21bc-11ea-a13a-137349068a90.jpg\n",
            " 92%|███████████████████████████████████████▊   | 37/40 [18:07<01:27, 29.03s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~89134a4a-21bc-11ea-a13a-137349068a90.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|████████████████████████████████████████▊  | 38/40 [18:38<00:59, 29.53s/it]Processing image /kaggle/working/images/8823b0b6-21bc-11ea-a13a-137349068a90.jpg\n",
            " 98%|█████████████████████████████████████████▉ | 39/40 [19:07<00:29, 29.22s/it]Processing image /kaggle/working/images/anno_~kaggle~working~images~890fff20-21bc-11ea-a13a-137349068a90.jpg\n",
            "100%|███████████████████████████████████████████| 40/40 [19:35<00:00, 29.38s/it]\n",
            "Finished inference in 19 minutes and 44.71 seconds\n",
            "Output file saved at /kaggle/working/output.json\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRB4Q9HUVvpq"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maKIELH5Vvpq"
      },
      "source": [
        "Mit [visualize_detector_output.py](https://github.com/microsoft/CameraTraps/blob/master/visualization/visualize_detector_output.py) aus dem Ordner visualization des Camera Traps Repository lassen sich die Ausgaben des Detectors auf unseren eingelesenen Bildern anzeigen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j_KPllZMVvpr"
      },
      "source": [
        "visualization_dir = '/kaggle/working/images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XZP3vqFPVvpr",
        "outputId": "6cb928a9-8b44-4ed1-8e04-6ecfd580ea16"
      },
      "source": [
        "!python visualize_detector_output.py \"$output_file_path\" \"$visualization_dir\" --confidence 0.8 --images_dir \"$images_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detection_categories provided\n",
            "Detector output file contains 40 entries.\n",
            "Starting to annotate the images...\n",
            "100%|███████████████████████████████████████████| 40/40 [00:02<00:00, 18.23it/s]\n",
            "Rendered detection results on 40 images, saved to /kaggle/working/images/.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sGkDHnXOVvpr"
      },
      "source": [
        "import os\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x0FpucVVvps"
      },
      "source": [
        "Hier werden anschließend jeweils die Bilder als Original, Ausschnitt und mit Detection (anno) ausgegeben.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iXL_IwdiVvps"
      },
      "source": [
        "for viz_file_name in os.listdir(visualization_dir):\n",
        "  #print(viz_file_name)\n",
        "  im = Image.open(os.path.join(visualization_dir, viz_file_name))\n",
        "  #display(im)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyoxK8OlVvps"
      },
      "source": [
        "![grafik.png](attachment:grafik.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R72d4PTwVvps"
      },
      "source": [
        "Aufgrund der beschränkten Kernel Source Größe findet man neben dem hier angezeigten Beispielbild auf GitHub im Ordner [detector](https://github.com/pds2021/capstone-group_2/tree/main/detector) weitere Beispielbilder zum Object Detector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bbjil7AVvpt"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SRBFIIaVvpt"
      },
      "source": [
        "Die Outputdatei [output.json](https://github.com/pds2021/capstone-group_2/blob/main/detector/output.json) enhält für jedes Bild den Dateinamen und die maximale Detection-Konfidenz, sowie die Boundingbox und Konfidenz pro erkanntes Tier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0klzolvfVvpt"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-SMYIFNxVvpt"
      },
      "source": [
        "with open(r'./output.json') as json_file:\n",
        "    data = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-xTuUtukVvpu",
        "outputId": "5ced0dc6-e6cc-4ccc-bed8-f515574bf414"
      },
      "source": [
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'detection_categories', 'info'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fiu0-5R5Vvpu",
        "outputId": "7a50bfa1-77b3-4484-d64e-efb6ce5cf97c"
      },
      "source": [
        "data['images']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'file': '/kaggle/working/images/anno_~kaggle~working~images~8c712ba8-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.11,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.11,\n",
              "    'bbox': [0.3448, 0.7357, 0.03945, 0.06361]}]},\n",
              " {'file': '/kaggle/working/images/905cdb0e-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.04335, 0.123, 0.1879, 0.7156]}]},\n",
              " {'file': '/kaggle/working/images/9310face-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.7618, 0.251, 0.2362, 0.3822]}]},\n",
              " {'file': '/kaggle/working/images/8e01cbc6-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~94cf8632-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.997,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.997,\n",
              "    'bbox': [0.223, 0.1565, 0.7687, 0.8108]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.904,\n",
              "    'bbox': [0.004457, 0.7508, 0.1841, 0.2368]},\n",
              "   {'category': '1', 'conf': 0.351, 'bbox': [0.2194, 0.4404, 0.499, 0.5316]},\n",
              "   {'category': '1', 'conf': 0.187, 'bbox': [0.2124, 0.09285, 0.5582, 0.7928]},\n",
              "   {'category': '1', 'conf': 0.167, 'bbox': [0.4195, 0.08868, 0.2771, 0.5492]},\n",
              "   {'category': '1', 'conf': 0.139, 'bbox': [0.006318, 0.7531, 0.102, 0.2277]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.124,\n",
              "    'bbox': [0.02636, 0.9295, 0.1639, 0.05546]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~89387e28-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.997,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.997,\n",
              "    'bbox': [0.4535, 0.7842, 0.1181, 0.1921]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~8e4efa86-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~9399b1ca-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.145, 0.1323, 0.8549, 0.8552]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.98,\n",
              "    'bbox': [0.002314, 0.7421, 0.1942, 0.2423]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~88eb0e86-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/873d13ae-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/9526e594-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~938b7ad8-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.996,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.996,\n",
              "    'bbox': [0.2029, 0.7346, 0.6456, 0.2476]},\n",
              "   {'category': '1', 'conf': 0.952, 'bbox': [0.008914, 0.562, 0.2172, 0.4133]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.76,\n",
              "    'bbox': [0.02918, 0.5773, 0.8084, 0.3931]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~8a3d4e2a-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~8e01cbc6-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.446,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.446,\n",
              "    'bbox': [0.2993, 0.6175, 0.07167, 0.1376]}]},\n",
              " {'file': '/kaggle/working/images/88eb0e86-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~957f9414-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.998,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.998,\n",
              "    'bbox': [0.6549, 0.5351, 0.1373, 0.1063]},\n",
              "   {'category': '1', 'conf': 0.269, 'bbox': [0.7566, 0.8425, 0.2345, 0.1317]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.216,\n",
              "    'bbox': [0.9306, 0.3395, 0.06859, 0.1415]}]},\n",
              " {'file': '/kaggle/working/images/8d238c30-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/94cf8632-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.992,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.992,\n",
              "    'bbox': [0.3944, 0.1735, 0.6055, 0.8024]},\n",
              "   {'category': '1', 'conf': 0.981, 'bbox': [0.2092, 0.4984, 0.4821, 0.4819]},\n",
              "   {'category': '1', 'conf': 0.972, 'bbox': [0.00122, 0.7509, 0.1929, 0.2384]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.815,\n",
              "    'bbox': [0.02574, 0.9267, 0.1728, 0.05813]},\n",
              "   {'category': '1', 'conf': 0.304, 'bbox': [0.3189, 0.1254, 0.4713, 0.7387]},\n",
              "   {'category': '1', 'conf': 0.241, 'bbox': [0.4207, 0.09293, 0.2924, 0.6062]},\n",
              "   {'category': '1', 'conf': 0.127, 'bbox': [0.4317, 0.04861, 0.1256, 0.4768]},\n",
              "   {'category': '1', 'conf': 0.124, 'bbox': [0.4317, 0.09086, 0.2422, 0.4151]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.121,\n",
              "    'bbox': [0.001943, 0.7538, 0.102, 0.2232]}]},\n",
              " {'file': '/kaggle/working/images/8e4efa86-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.969,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.969,\n",
              "    'bbox': [0.9757, 0.6999, 0.0232, 0.07493]}]},\n",
              " {'file': '/kaggle/working/images/89387e28-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.4557, 0.7788, 0.1192, 0.2017]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.897,\n",
              "    'bbox': [0.1804, 0.6868, 0.07558, 0.1215]}]},\n",
              " {'file': '/kaggle/working/images/8a3d4e2a-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~926f87e8-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.008982, 0.5583, 0.5478, 0.4308]}]},\n",
              " {'file': '/kaggle/working/images/890fff20-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/957f9414-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.6447, 0.5311, 0.1557, 0.1182]},\n",
              "   {'category': '1', 'conf': 0.894, 'bbox': [0.7415, 0.8531, 0.2584, 0.1263]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.225,\n",
              "    'bbox': [0.9307, 0.337, 0.06897, 0.1544]}]},\n",
              " {'file': '/kaggle/working/images/938b7ad8-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.992,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.992,\n",
              "    'bbox': [0.1805, 0.7327, 0.6805, 0.2525]},\n",
              "   {'category': '1', 'conf': 0.916, 'bbox': [0.000343, 0.5608, 0.22, 0.424]},\n",
              "   {'category': '1', 'conf': 0.753, 'bbox': [0.09425, 0.6242, 0.7401, 0.3459]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.226,\n",
              "    'bbox': [0.03047, 0.5541, 0.6094, 0.4353]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~875e1de2-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~8823b0b6-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/89134a4a-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~905cdb0e-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.995,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.995,\n",
              "    'bbox': [0.03755, 0.1349, 0.1816, 0.6872]}]},\n",
              " {'file': '/kaggle/working/images/926f87e8-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.0006241, 0.5528, 0.6314, 0.438]},\n",
              "   {'category': '1', 'conf': 0.355, 'bbox': [0.5144, 0.8588, 0.13, 0.1214]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.193,\n",
              "    'bbox': [0.3945, 0.8619, 0.2394, 0.1218]}]},\n",
              " {'file': '/kaggle/working/images/8c712ba8-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~9526e594-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/9399b1ca-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.1403, 0.1247, 0.8496, 0.873]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.967,\n",
              "    'bbox': [0.001139, 0.7387, 0.1952, 0.2489]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.732,\n",
              "    'bbox': [0.02657, 0.9214, 0.1712, 0.06347]},\n",
              "   {'category': '1',\n",
              "    'conf': 0.134,\n",
              "    'bbox': [0.001376, 0.7458, 0.1017, 0.2324]}]},\n",
              " {'file': '/kaggle/working/images/875e1de2-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.944,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.944,\n",
              "    'bbox': [0.8903, 0.7079, 0.1083, 0.1689]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~8d238c30-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.661,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.661,\n",
              "    'bbox': [0.1769, 0.5558, 0.173, 0.1298]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~873d13ae-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~9310face-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0.999,\n",
              "  'detections': [{'category': '1',\n",
              "    'conf': 0.999,\n",
              "    'bbox': [0.7772, 0.2593, 0.2176, 0.3807]}]},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~89134a4a-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/8823b0b6-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []},\n",
              " {'file': '/kaggle/working/images/anno_~kaggle~working~images~890fff20-21bc-11ea-a13a-137349068a90.jpg',\n",
              "  'max_detection_conf': 0,\n",
              "  'detections': []}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7iakjgN8Vvpv",
        "outputId": "2c3cc21e-1b4a-4cb1-9fd0-c4ed14e78c26"
      },
      "source": [
        "data['detection_categories']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1': 'animal', '2': 'person', '3': 'vehicle'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5ROjUMv5Vvpw",
        "outputId": "9b6158cd-3dda-4521-83fa-16453881c058"
      },
      "source": [
        "data['info']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'detection_completion_time': '2021-02-23 14:55:09', 'format_version': '1.0'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfByJL8aVvpx"
      },
      "source": [
        "Aufgrund der langen Bearbeitungszeit des Object Detectors haben wir uns dafür entschieden, für die weitere Bearbeitung des Projektes mit den fertigen Dateien der Kaggle Challenge zu arbeiten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtLe0xTOVvpx"
      },
      "source": [
        "Quellen:\n",
        "* https://github.com/microsoft/CameraTraps/tree/master/detection\n",
        "* https://github.com/microsoft/CameraTraps/tree/master/visualization\n",
        "* https://paperswithcode.com/method/inception-resnet-v2"
      ]
    }
  ]
}